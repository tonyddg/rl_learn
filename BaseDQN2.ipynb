{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本 DQN\n",
    "> 参考自 <https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html>\n",
    "\n",
    "## 概述\n",
    "* 包含 Double DQN 与经验回放的 DQN\n",
    "* 模型实现代码 `./dqn/BaseDQN2.py`\n",
    "* 测试环境 gymnasium CartPole-v1\n",
    "\n",
    "## 记录\n",
    "* v2.0\n",
    "    * 相比于 v1.1, 更加符合标准 DQN 算法, 在模型 CartPole-v1 中收敛\n",
    "    * 按 Episode 软更新 Target Network \n",
    "    * 按 action 的调用更新 epsilon, 更加合理\n",
    "    * 使用了 AdamW 优化器 (开启 amsgrad) 以及裁剪梯度\n",
    "    * 使用了更符合 CartPole-v1 的模型超参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dqn.BaseDQN2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(name: str, comment: str, episode: int = 500, hparam: HyperParam | None = None, is_write: bool = True):\n",
    "    '''\n",
    "    * `name` 训练名称\n",
    "    * `comment` 训练注释\n",
    "    * `episode` 训练片段数\n",
    "    * `hparam` 超参数\n",
    "    * `is_write` 是否记录训练数据\n",
    "    '''\n",
    "    env = gym.make(\n",
    "        \"CartPole-v1\", \n",
    "        render_mode = \"rgb_array\"\n",
    "    )\n",
    "    if is_write:\n",
    "        env = RecordEpisodeStatistics(env, buffer_length = 1)\n",
    "        env = RecordVideo(\n",
    "            env, \n",
    "            video_folder = \"vedio_CartPole_with_BaseDQN\", \n",
    "            name_prefix = name,\n",
    "            episode_trigger = lambda x: (x + 1) % 100 == 0\n",
    "        )\n",
    "\n",
    "    if hparam == None:\n",
    "        hparam = HyperParam()\n",
    "    model = BaseDQN(hparam)\n",
    "\n",
    "    writer = None\n",
    "    if is_write:\n",
    "        writer = SummaryWriter(comment = name + \"_\" + comment)\n",
    "\n",
    "    for episode in tqdm(range(episode)):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        total_loss = 0\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            # 完成一次状态转移\n",
    "            action = model.take_action_single(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action[0])\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # 更新模型\n",
    "            transition = make_transition_from_numpy(state, action, next_state, reward, terminated)\n",
    "            loss = model.update(transition)\n",
    "            if loss != False:\n",
    "                total_loss += loss\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        model.update_episode(episode)\n",
    "\n",
    "        # tensorboard 记录平均损失与累计回报\n",
    "        if writer != None:        \n",
    "            writer.add_scalar(\n",
    "                f\"{name}/avg_loss\",\n",
    "                total_loss / info[\"episode\"][\"l\"],\n",
    "                episode\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                f\"{name}/return\",\n",
    "                info[\"episode\"][\"r\"],\n",
    "                episode\n",
    "            )\n",
    "\n",
    "        # 记录动作倾向\n",
    "        if writer != None:  \n",
    "            if episode % 50 == 0:\n",
    "                action_sum = 0\n",
    "                for i in model.reply_queue.buffer:\n",
    "                    action_sum += i.action.item()\n",
    "\n",
    "                writer.add_scalar(\n",
    "                    f\"{name}/avg_action\",\n",
    "                    action_sum / model.reply_queue.size(),\n",
    "                    int(episode / 50)\n",
    "                )\n",
    "    env.close()\n",
    "    \n",
    "    if writer != None:\n",
    "        writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\"CartPole-v1\", \"test\", 600, is_write = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行结果\n",
    "\n",
    "## v2.0\n",
    "![](./res/CartPole_v1_BaseDQN_v2_0.png)\n",
    "\n",
    "视频见 `./res/CartPole_v1_BaseDQN_v2_0.zip`\n",
    "\n",
    "## todo\n",
    "* 优化代码, 计算 TD 目标时, 当 done 为 True 时不进行预测\n",
    "* 矢量化环境中, 决策可能存在问题"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
